import requests
import time
import re
from urllib.parse import urlparse, parse_qs
from datetime import datetime

import os
import sys
import logging
from typing import Any, Dict, List, Optional

import pymysql
from dotenv import load_dotenv, find_dotenv

load_dotenv(find_dotenv())

def env_required(key: str) -> str:
    v = os.getenv(key)
    if not v:
        raise RuntimeError(f"í™˜ê²½ ë³€ìˆ˜ {key}ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    return v

def env_int(key: str, default: Optional[int] = None) -> int:
    v = os.getenv(key)
    if v is None:
        if default is None:
            raise RuntimeError(f"í™˜ê²½ ë³€ìˆ˜ {key}ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        return default
    return int(v)

ACCESS_KEY = env_required("ACCESS_KEY")
BASE_URL = 'https://oapi.saramin.co.kr/job-search'
HEADERS = {'Accept': 'application/json'}

REGION_CODE_MAP = {
    '101': 'ì„œìš¸',
    '102': 'ê²½ê¸°',
    '103': 'ê´‘ì£¼',
    '104': 'ëŒ€êµ¬',
    '105': 'ëŒ€ì „',
    '106': 'ë¶€ì‚°',
    '107': 'ìš¸ì‚°',
    '108': 'ì¸ì²œ',
    '109': 'ê°•ì›',
    '110': 'ê²½ë‚¨',
    '111': 'ê²½ë¶',
    '112': 'ì „ë‚¨',
    '113': 'ì „ë¶',
    '114': 'ì¶©ë¶',
    '115': 'ì¶©ë‚¨',
    '116': 'ì œì£¼',
    '117': 'ì „êµ­',
    '118': 'ì„¸ì¢…'
    # 2XX: í•´ì™¸ â†’ ì œì™¸ ëŒ€ìƒ
}

INDUSTRY_CODE_MAP = {
    '1': 'ì„œë¹„ìŠ¤ì—…',
    '2': 'ì œì¡°,í™”í•™',
    '3': 'IT,ì›¹,í†µì‹ ',
    '4': 'ì€í–‰,ê¸ˆìœµì—…',
    '5': 'ë¯¸ë””ì–´,ë””ìì¸',
    '6': 'êµìœ¡ì—…',
    '7': 'ì˜ë£Œ,ì œì•½,ë³µì§€',
    '8': 'íŒë§¤,ìœ í†µ',
    '9': 'ê±´ì„¤ì—…',
    '10': 'ê¸°ê´€,í˜‘íšŒ'
}

job_code_to_category = {
    80: 'ê²Œì„/ê·¸ë˜í”½', 81: 'ê¸°íƒ€/ê³µí†µ', 82: 'ë°ì´í„°/AI', 83: 'ë°ì´í„°/AI', 84: 'ë°±ì—”ë“œ', 85: 'ë³´ì•ˆ/ì¸í”„ë¼',
    86: 'ëª¨ë°”ì¼', 87: 'í”„ë¡ íŠ¸ì—”ë“œ', 88: 'í”„ë¡ íŠ¸ì—”ë“œ', 89: 'ê¸°íƒ€/ê³µí†µ', 90: 'ë³´ì•ˆ/ì¸í”„ë¼', 91: 'í”„ë¡ íŠ¸ì—”ë“œ',
    92: 'í”„ë¡ íŠ¸ì—”ë“œ', 93: 'ë³´ì•ˆ/ì¸í”„ë¼', 94: 'ê¸°íƒ€/ê³µí†µ', 95: 'ë°±ì—”ë“œ', 96: 'ê¸°íƒ€/ê³µí†µ', 97: 'ê²Œì„/ê·¸ë˜í”½',
    98: 'ê¸°íƒ€/ê³µí†µ', 99: 'ê¸°íƒ€/ê³µí†µ', 100: 'ë³´ì•ˆ/ì¸í”„ë¼', 101: 'ë°±ì—”ë“œ', 102: 'ê¸°íƒ€/ê³µí†µ', 103: 'ê¸°íƒ€/ê³µí†µ',
    104: 'ë°±ì—”ë“œ', 106: 'ë°ì´í„°/AI', 107: 'ë°ì´í„°/AI', 108: 'ë°ì´í„°/AI', 109: 'ë°ì´í„°/AI', 110: 'ê²Œì„/ê·¸ë˜í”½',
    111: 'ë³´ì•ˆ/ì¸í”„ë¼', 112: 'ë°±ì—”ë“œ', 113: 'í”„ë¡ íŠ¸ì—”ë“œ', 114: 'ë³´ì•ˆ/ì¸í”„ë¼', 115: 'ê¸°íƒ€/ê³µí†µ', 116: 'ë°ì´í„°/AI',
    117: 'ê¸°íƒ€/ê³µí†µ', 118: 'ê¸°íƒ€/ê³µí†µ', 119: 'ë°±ì—”ë“œ', 120: 'ë°ì´í„°/AI', 121: 'ë³´ì•ˆ/ì¸í”„ë¼', 122: 'ë°ì´í„°/AI',
    123: 'ë°ì´í„°/AI', 124: 'í”„ë¡ íŠ¸ì—”ë“œ', 125: 'ë°ì´í„°/AI', 126: 'ë°ì´í„°/AI', 127: 'DevOps/í´ë¼ìš°ë“œ',
    128: 'ì„ë² ë””ë“œ/í•˜ë“œì›¨ì–´', 129: 'ê¸°íƒ€/ê³µí†µ', 130: 'ê¸°íƒ€/ê³µí†µ', 131: 'ë°ì´í„°/AI', 132: 'ë³´ì•ˆ/ì¸í”„ë¼',
    133: 'ê¸°íƒ€/ê³µí†µ', 134: 'í”„ë¡ íŠ¸ì—”ë“œ', 135: 'ê¸°íƒ€/ê³µí†µ', 136: 'DevOps/í´ë¼ìš°ë“œ', 137: 'ë°ì´í„°/AI',
    138: 'ê¸°íƒ€/ê³µí†µ', 139: 'ì„ë² ë””ë“œ/í•˜ë“œì›¨ì–´', 140: 'ê¸°íƒ€/ê³µí†µ', 141: 'ê¸°íƒ€/ê³µí†µ', 142: 'ê¸°íƒ€/ê³µí†µ',
    143: 'ê¸°íƒ€/ê³µí†µ', 144: 'ê²Œì„/ê·¸ë˜í”½', 145: 'ë°±ì—”ë“œ', 146: 'DevOps/í´ë¼ìš°ë“œ', 147: 'ë³´ì•ˆ/ì¸í”„ë¼',
    148: 'ê¸°íƒ€/ê³µí†µ', 149: 'ê¸°íƒ€/ê³µí†µ', 150: 'ê¸°íƒ€/ê³µí†µ', 151: 'ì„ë² ë””ë“œ/í•˜ë“œì›¨ì–´', 152: 'ê¸°íƒ€/ê³µí†µ',
    153: 'ì„ë² ë””ë“œ/í•˜ë“œì›¨ì–´', 154: 'ê¸°íƒ€/ê³µí†µ', 155: 'ê¸°íƒ€/ê³µí†µ', 157: 'ë³´ì•ˆ/ì¸í”„ë¼', 158: 'ì„ë² ë””ë“œ/í•˜ë“œì›¨ì–´',
    159: 'ê¸°íƒ€/ê³µí†µ', 160: 'ë°ì´í„°/AI', 161: 'ë°ì´í„°/AI', 162: 'ë°ì´í„°/AI', 163: 'ë°ì´í„°/AI',
    164: 'ë°±ì—”ë“œ', 165: 'ê¸°íƒ€/ê³µí†µ', 166: 'ì„ë² ë””ë“œ/í•˜ë“œì›¨ì–´', 167: 'ê¸°íƒ€/ê³µí†µ', 168: 'ê¸°íƒ€/ê³µí†µ',
    169: 'ê¸°íƒ€/ê³µí†µ', 170: 'ë°±ì—”ë“œ', 171: 'ê¸°íƒ€/ê³µí†µ', 172: 'ê¸°íƒ€/ê³µí†µ', 173: 'ë³´ì•ˆ/ì¸í”„ë¼', 174: 'ê¸°íƒ€/ê³µí†µ',
    175: 'ê¸°íƒ€/ê³µí†µ', 176: 'ê¸°íƒ€/ê³µí†µ', 177: 'ë³´ì•ˆ/ì¸í”„ë¼', 178: 'ê²Œì„/ê·¸ë˜í”½', 179: 'ê¸°íƒ€/ê³µí†µ',
    180: 'ê¸°íƒ€/ê³µí†µ', 181: 'ë°ì´í„°/AI', 182: 'ê¸°íƒ€/ê³µí†µ', 183: 'ê¸°íƒ€/ê³µí†µ', 184: 'ê¸°íƒ€/ê³µí†µ',
    185: 'ê¸°íƒ€/ê³µí†µ', 186: 'ì„ë² ë””ë“œ/í•˜ë“œì›¨ì–´', 187: 'ê¸°íƒ€/ê³µí†µ', 188: 'ê¸°íƒ€/ê³µí†µ', 189: 'ê¸°íƒ€/ê³µí†µ',
    190: 'ë³´ì•ˆ/ì¸í”„ë¼', 191: 'ê¸°íƒ€/ê³µí†µ', 192: 'ê¸°íƒ€/ê³µí†µ', 193: 'ê¸°íƒ€/ê³µí†µ', 194: 'ê¸°íƒ€/ê³µí†µ',
    195: 'ëª¨ë°”ì¼', 196: 'ëª¨ë°”ì¼', 197: 'ê¸°íƒ€/ê³µí†µ', 198: 'ê¸°íƒ€/ê³µí†µ', 199: 'ê¸°íƒ€/ê³µí†µ', 200: 'ê¸°íƒ€/ê³µí†µ',
    201: 'DevOps/í´ë¼ìš°ë“œ', 202: 'DevOps/í´ë¼ìš°ë“œ', 203: 'ê¸°íƒ€/ê³µí†µ', 204: 'ë°±ì—”ë“œ', 205: 'ë°±ì—”ë“œ',
    206: 'ë°±ì—”ë“œ', 207: 'ê¸°íƒ€/ê³µí†µ', 208: 'ê¸°íƒ€/ê³µí†µ', 209: 'í”„ë¡ íŠ¸ì—”ë“œ', 210: 'í”„ë¡ íŠ¸ì—”ë“œ', 211: 'ê¸°íƒ€/ê³µí†µ',
    212: 'ê¸°íƒ€/ê³µí†µ', 213: 'ê¸°íƒ€/ê³µí†µ', 214: 'DevOps/í´ë¼ìš°ë“œ', 215: 'ê¸°íƒ€/ê³µí†µ', 216: 'ê¸°íƒ€/ê³µí†µ',
    217: 'DevOps/í´ë¼ìš°ë“œ', 218: 'ê¸°íƒ€/ê³µí†µ', 219: 'ê¸°íƒ€/ê³µí†µ', 220: 'ëª¨ë°”ì¼', 221: 'DevOps/í´ë¼ìš°ë“œ',
    222: 'ê¸°íƒ€/ê³µí†µ', 223: 'ê¸°íƒ€/ê³µí†µ', 224: 'ê¸°íƒ€/ê³µí†µ', 225: 'ê¸°íƒ€/ê³µí†µ', 226: 'ê¸°íƒ€/ê³µí†µ', 227: 'ê¸°íƒ€/ê³µí†µ',
    228: 'ê¸°íƒ€/ê³µí†µ', 229: 'í”„ë¡ íŠ¸ì—”ë“œ', 230: 'ë°±ì—”ë“œ', 231: 'ê¸°íƒ€/ê³µí†µ', 232: 'ê¸°íƒ€/ê³µí†µ', 233: 'ëª¨ë°”ì¼',
    234: 'ëª¨ë°”ì¼', 235: 'ë°±ì—”ë“œ', 236: 'í”„ë¡ íŠ¸ì—”ë“œ', 237: 'ê¸°íƒ€/ê³µí†µ', 238: 'ë°±ì—”ë“œ', 239: 'í”„ë¡ íŠ¸ì—”ë“œ',
    240: 'ë°±ì—”ë“œ', 241: 'ê¸°íƒ€/ê³µí†µ', 242: 'ê¸°íƒ€/ê³µí†µ', 243: 'ëª¨ë°”ì¼', 244: 'DevOps/í´ë¼ìš°ë“œ',
    245: 'ê¸°íƒ€/ê³µí†µ', 246: 'ê¸°íƒ€/ê³µí†µ', 247: 'ë³´ì•ˆ/ì¸í”„ë¼', 248: 'ê¸°íƒ€/ê³µí†µ', 249: 'ê¸°íƒ€/ê³µí†µ',
    250: 'ê¸°íƒ€/ê³µí†µ', 251: 'ê¸°íƒ€/ê³µí†µ', 252: 'ê¸°íƒ€/ê³µí†µ', 253: 'ê¸°íƒ€/ê³µí†µ', 254: 'ê¸°íƒ€/ê³µí†µ',
    255: 'ê¸°íƒ€/ê³µí†µ', 256: 'ë°±ì—”ë“œ', 257: 'ë°±ì—”ë“œ', 258: 'ëª¨ë°”ì¼', 259: 'ê¸°íƒ€/ê³µí†µ', 260: 'ëª¨ë°”ì¼',
    261: 'ê¸°íƒ€/ê³µí†µ', 262: 'ê¸°íƒ€/ê³µí†µ', 263: 'ê¸°íƒ€/ê³µí†µ', 264: 'ê¸°íƒ€/ê³µí†µ', 265: 'ê¸°íƒ€/ê³µí†µ',
    266: 'ë°ì´í„°/AI', 267: 'ê¸°íƒ€/ê³µí†µ', 268: 'ë°±ì—”ë“œ', 269: 'ë°±ì—”ë“œ', 270: 'ë°±ì—”ë“œ', 271: 'ê¸°íƒ€/ê³µí†µ',
    272: 'ë°±ì—”ë“œ', 273: 'ë°ì´í„°/AI', 274: 'ë°ì´í„°/AI', 275: 'ê¸°íƒ€/ê³µí†µ', 276: 'ë°ì´í„°/AI', 277: 'í”„ë¡ íŠ¸ì—”ë“œ',
    278: 'ëª¨ë°”ì¼', 279: 'ëª¨ë°”ì¼', 280: 'ë°±ì—”ë“œ', 281: 'ê¸°íƒ€/ê³µí†µ', 282: 'ë°±ì—”ë“œ', 283: 'ë°±ì—”ë“œ',
    284: 'ê¸°íƒ€/ê³µí†µ', 285: 'ë°ì´í„°/AI', 286: 'ë°±ì—”ë“œ', 287: 'ë°±ì—”ë“œ', 288: 'ê¸°íƒ€/ê³µí†µ', 289: 'ë°ì´í„°/AI',
    290: 'ë³´ì•ˆ/ì¸í”„ë¼', 291: 'ë°±ì—”ë“œ', 292: 'ë°±ì—”ë“œ', 293: 'ë°±ì—”ë“œ', 294: 'ë°±ì—”ë“œ', 295: 'ë°±ì—”ë“œ',
    296: 'ë°±ì—”ë“œ', 297: 'ê¸°íƒ€/ê³µí†µ', 298: 'ëª¨ë°”ì¼', 299: 'ê¸°íƒ€/ê³µí†µ', 300: 'ë°ì´í„°/AI', 301: 'ê¸°íƒ€/ê³µí†µ',
    302: 'ê¸°íƒ€/ê³µí†µ', 303: 'ê¸°íƒ€/ê³µí†µ', 304: 'ê²Œì„/ê·¸ë˜í”½', 305: 'ê¸°íƒ€/ê³µí†µ', 306: 'ê²Œì„/ê·¸ë˜í”½',
    307: 'ê¸°íƒ€/ê³µí†µ', 308: 'ê¸°íƒ€/ê³µí†µ', 309: 'ê¸°íƒ€/ê³µí†µ', 310: 'ê¸°íƒ€/ê³µí†µ', 311: 'ê¸°íƒ€/ê³µí†µ',
    312: 'í”„ë¡ íŠ¸ì—”ë“œ', 313: 'ê¸°íƒ€/ê³µí†µ', 314: 'í”„ë¡ íŠ¸ì—”ë“œ', 315: 'í”„ë¡ íŠ¸ì—”ë“œ', 316: 'ê¸°íƒ€/ê³µí†µ',
    317: 'ê¸°íƒ€/ê³µí†µ', 318: 'ê¸°íƒ€/ê³µí†µ', 319: 'ì„ë² ë””ë“œ/í•˜ë“œì›¨ì–´', 320: 'ì„ë² ë””ë“œ/í•˜ë“œì›¨ì–´'
}


def clean_industry_codes(raw_industry_list):
    """ì—…ì¢… ë¦¬ìŠ¤íŠ¸ì—ì„œ ìƒìœ„ ì—…ì¢… ì½”ë“œë§Œ ì¶”ì¶œ (ë’¤ 2ìë¦¬ ì œê±° í›„ ë§¤í•‘)"""
    industries = set()

    for ind in raw_industry_list:
        code = ind.get('code')
        if not code or len(code) < 3:
            continue

        # ë’¤ 2ìë¦¬ ì œê±° â†’ ìƒìœ„ ì—…ì¢… ì½”ë“œ
        upper_code = code[:-2]  # ex: '303' â†’ '3', '1001' â†’ '10'

        name = INDUSTRY_CODE_MAP.get(upper_code)
        if name:
            industries.add(name)

    return ','.join(sorted(industries)) if industries else None


def clean_region_codes(raw_location_list):
    """APIë¡œ ë°›ì€ ë³µìˆ˜ ì§€ì—­(ì½¤ë§ˆ í¬í•¨ ê°€ëŠ¥)ì—ì„œ êµ­ë‚´ 1ì°¨ ì§€ì—­(3ìë¦¬ prefix)ë§Œ ì¶”ì¶œ"""
    region_prefixes = set()

    # ì…ë ¥ì´ dictë¡œ ì˜¤ëŠ” ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ í†µì¼
    if isinstance(raw_location_list, dict):
        raw_location_list = [raw_location_list]
    elif raw_location_list is None:
        raw_location_list = []

    for loc in raw_location_list:
        code_field = loc.get('code')
        if not code_field:
            continue

        # code_fieldê°€ "101050,101060,101070"ì²˜ëŸ¼ ì½¤ë§ˆë¡œ ë¬¶ì—¬ ìˆì„ ìˆ˜ ìˆìŒ
        for code in str(code_field).split(','):
            code = code.strip()
            if len(code) < 6 or not code.isdigit():
                continue
            prefix = code[:3]  # 1ì°¨ ì§€ì—­
            # í•´ì™¸(2ë¡œ ì‹œì‘í•˜ëŠ” ëŒ€ë¥™ ì½”ë“œ) ì œì™¸
            if prefix.startswith('2'):
                continue
            if prefix in REGION_CODE_MAP:
                region_prefixes.add(prefix)

    regions = [REGION_CODE_MAP[p] for p in sorted(region_prefixes)]
    return ','.join(regions) if regions else None


def extract_csn(href):
    if not href:
        return None
    try:
        parsed = urlparse(href)
        qs = parse_qs(parsed.query)
        csn = qs.get('csn', [None])[0]
        if csn:
            match = re.search(r'\d{10}', csn)
            return match.group() if match else None
    except Exception:
        return None

def timestamp_to_datetime_str(ts):
    try:
        return datetime.fromtimestamp(int(ts)).strftime('%Y-%m-%d %H:%M:%S')
    except:
        return None

def clean_to_string(value):
    if isinstance(value, list):
        return ','.join(value)
    return str(value) if value else None

def fetch_and_clean_jobs():
    page = 0
    max_count = 110
    cleaned_jobs = []

    while True:
        params = {
            'access-key': ACCESS_KEY,
            'job_mid_cd': 2,
            'stock': 'kospi,kosdaq,konex',
            'count': max_count,
            'start': page,
            'sort': 'pd'
        }

        print(f"ğŸ“¡ ìš”ì²­ ì¤‘... page={page}")
        response = requests.get(BASE_URL, headers=HEADERS, params=params)
        if response.status_code != 200:
            print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
            break

        data = response.json().get('jobs', {})
        jobs = data.get('job', [])
        total = int(data.get('total', 0))
        print(f"âœ… ë°›ì€ ê³µê³  ìˆ˜: {len(jobs)} / ì „ì²´: {total}")

        if not jobs:
            print("ğŸ›‘ ë” ì´ìƒ ê°€ì ¸ì˜¬ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
            break

        for job in jobs:
            try:
                exp_code = job['position']['experience-level']['code']
                if str(exp_code) not in ('1', '3', '0'):  # ì‹ ì… ì¡°ê±´
                    continue

                position = job.get('position', {})
                company = job.get('company', {}).get('detail', {})

                # ì‚¬ì—…ì ë“±ë¡ë²ˆí˜¸ â†’ DB ì¡°íšŒ í›„ company_idë¡œ ë§¤í•‘ í•„ìš” (ì—¬ê¸°ì„  ì¶”ì¶œë§Œ)
                csn = extract_csn(company.get('href'))

                # ğŸ”¸ ì§€ì—­ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬ (list of dicts)
                location_list = position.get('location', [])
                if isinstance(location_list, dict):
                  location_list = [location_list]

                #ì—…ì¢… ì²˜ë¦¬
                industry_list = position.get('industry', [])
                if isinstance(industry_list, dict):
                  industry_list = [industry_list]

                # ì§ë¬´ ì½”ë“œ â†’ ëŒ€ë¶„ë¥˜ ë§¤í•‘
                job_category = None

                try:
                    job_code_str = job.get("position", {}).get("job-code", {}).get("code")

                    if not job_code_str:
                        raise ValueError("ì§ë¬´ ì½”ë“œê°€ ì—†ìŒ")

                    job_categories = set()
                    for code_str in job_code_str.split(','):
                        code_str = code_str.strip()
                        if not code_str.isdigit():
                            continue
                        code = int(code_str)
                        category = job_code_to_category.get(code)
                        if category:
                            job_categories.add(category)

                    job_category = ",".join(sorted(job_categories)) if job_categories else None

                except Exception as e:
                    print(f"âš ï¸ ì§ë¬´ ì½”ë“œ íŒŒì‹± ì‹¤íŒ¨: {e}")
                    job_category = None


                # ì •ì œëœ ë°ì´í„° ìƒì„±
                cleaned_jobs.append({
                    'company_id': csn,  # ì‹¤ì œ ì‚¬ìš© ì‹œ DB ì¡°íšŒ í•„ìš”
                    'company_name': company.get('name'),
                    'title': position.get('title'),
                    'industry': clean_industry_codes(industry_list),
                    'job_category': job_category,
                    'region': clean_region_codes(location_list),
                    'work_type': clean_to_string(position.get('job-type', {}).get('name')),
                    'education_level': position.get('required-education-level', {}).get('name'),
                    'opening_date': timestamp_to_datetime_str(job.get('opening-timestamp')),
                    'expiration_date': timestamp_to_datetime_str(job.get('expiration-timestamp')),
                    'external_job_id': str(job.get('id'))
                })

            except Exception as e:
                print(f"âš ï¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue

        page += 1
        time.sleep(1.0)

    return cleaned_jobs

# -----------------------------------------------------------------------------
# í™˜ê²½ì„¤ì •
# -----------------------------------------------------------------------------


MYSQL_HOST     = env_required("MYSQL_HOST")
MYSQL_PORT     = env_int("MYSQL_PORT", 3306)
MYSQL_USER     = env_required("MYSQL_USER")
MYSQL_PASSWORD = env_required("MYSQL_PASSWORD")
MYSQL_DB       = env_required("MYSQL_DB")
MYSQL_CHARSET  = os.getenv("MYSQL_CHARSET", "utf8mb4")
#MYSQL_SSL_CA   = os.getenv("MYSQL_SSL_CA")


logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)],
)
logger = logging.getLogger("job_saver")

# -----------------------------------------------------------------------------
# DB ì—°ê²°
# -----------------------------------------------------------------------------
def get_conn():
    kwargs = dict(
        host        = MYSQL_HOST,
        port        = MYSQL_PORT,
        user        = MYSQL_USER,
        password    = MYSQL_PASSWORD,
        database    = MYSQL_DB,
        charset     = MYSQL_CHARSET,
        autocommit  = True,
        cursorclass = pymysql.cursors.DictCursor,
    )

    # if MYSQL_SSL_CA:
    #    kwargs["ssl"] = {"ca": MYSQL_SSL_CA}

    return pymysql.connect(**kwargs)

# -----------------------------------------------------------------------------
# ìœ í‹¸: company_id ì¡°íšŒ (ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸ -> company.id)
# -----------------------------------------------------------------------------
def _to_date_or_none(dt_str: Optional[str]) -> Optional[str]:
    """'YYYY-MM-DD HH:MM:SS' ë˜ëŠ” 'YYYY-MM-DD' -> 'YYYY-MM-DD' (DATE ì»¬ëŸ¼ìš©)"""
    if not dt_str:
        return None
    return dt_str[:19]

def get_company_id_by_business_no(conn, business_no: str) -> Optional[int]:
    if not business_no:
        return None
    sql = "SELECT id FROM company WHERE business_no = %s"
    with conn.cursor() as cur:
        cur.execute(sql, (business_no,))
        row = cur.fetchone()
        return row["id"] if row else None

def save_cleaned_jobs(cleaned_jobs: List[Dict[str, Any]], get_conn_func) -> Dict[str, int]:
    """
    cleaned_jobs ì˜ˆì‹œ í‚¤:
      company_id(=ì‚¬ì—…ìë²ˆí˜¸), company_name, title, industry, job_category,
      region, work_type, education_level, opening_date, expiration_date
    """
    stats = {"inserted": 0, "skipped_no_company": 0, "skipped_missing_fields": 0, "errors": 0}

    insert_sql = """
        INSERT INTO job (
            title,
            company_name,
            region,
            industry,
            job_category,
            work_type,
            education_level,
            opening_date,
            expiration_date,
            created_at,
            updated_at,
            company_id,
            external_job_id
        ) VALUES (
            %(title)s,
            %(company_name)s,
            %(region)s,
            %(industry)s,
            %(job_category)s,
            %(work_type)s,
            %(education_level)s,
            %(opening_date)s,
            %(expiration_date)s,
            CURDATE(),
            CURDATE(),
            %(company_id)s,
            %(external_job_id)s
        )
        ON DUPLICATE KEY UPDATE
            company_name    = VALUES(company_name),
            region          = VALUES(region),
            industry        = VALUES(industry),
            job_category    = VALUES(job_category),
            work_type       = VALUES(work_type),
            education_level = VALUES(education_level),
            opening_date    = VALUES(opening_date),
            expiration_date = VALUES(expiration_date),
            updated_at      = CURDATE();
    """


    with get_conn_func() as conn, conn.cursor() as cur:
        for idx, j in enumerate(cleaned_jobs, start=1):
            try:
                biz_no = j.get("company_id")   # ì •ì œë‹¨ê³„ì—ì„œ csn(ì‚¬ì—…ìë²ˆí˜¸)ì´ ë“¤ì–´ìˆìŒ
                title  = j.get("title")
                if not biz_no or not title:
                    stats["skipped_missing_fields"] += 1
                    logger.warning("í–‰ %d ìŠ¤í‚µ: í•„ìˆ˜ê°’ ëˆ„ë½(business_no/title). data=%s", idx, j)
                    continue

                cid = get_company_id_by_business_no(conn, biz_no)
                if not cid:
                    stats["skipped_no_company"] += 1
                    logger.warning("í–‰ %d ìŠ¤í‚µ: company(business_no=%s) ë¯¸ì¡´ì¬", idx, biz_no)
                    continue

                params = {
                    "company_id": cid,
                    "title": title,
                    "company_name": j.get("company_name"),
                    "region": j.get("region"),
                    "industry": j.get("industry"),
                    "job_category": j.get("job_category"),
                    "work_type": j.get("work_type"),
                    "education_level": j.get("education_level"),
                    "opening_date": _to_date_or_none(j.get("opening_date")),
                    "expiration_date": _to_date_or_none(j.get("expiration_date")),
                    "external_job_id": j.get("external_job_id")
                }

                cur.execute(insert_sql, params)
                stats["inserted"] += 1

            except Exception as e:
                stats["errors"] += 1
                logger.exception("í–‰ %d ì €ì¥ ì‹¤íŒ¨: %s", idx, e)

    logger.info("ì €ì¥ ìš”ì•½: %s", stats)
    return stats


if __name__ == "__main__":
    job_data = fetch_and_clean_jobs()
    save_cleaned_jobs(job_data, get_conn)